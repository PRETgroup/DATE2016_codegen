\section{Benchmarking}
\label{sec:benchmarking}


We present a set of experiments to evaluate the efficacy of the proposed
modular code generation tool (\ourTool) with \simulink.  In the first 
experiment, we evaluate the \emph{scalability} of \ourTool and \simulink as the 
number of nodes in the \ac{NHN} model increases.  In the second experiment, we 
select benchmarks that span across different application domains such as 
medical, physics, and industrial automation to illustrate the \emph{diversity} 
of the proposed approach.  We compare each of these benchmarks against 
\simulink in terms of execution time and maximum memory usage.  


\subsection{Experimental set-up}
\label{sec:experimentalSetUp}
The following aspects were considered in order to achieve a fair
comparison between \ourTool and \simulink:

\begin{description}[\IEEEsetlabelwidth{Step Size}\IEEEusemathlabelsep]
\item[\textbf{Solver}] To reflect the synchronous execution model, we
  used a discrete numerical solver with a fixed step in \simulink,
  namely \texttt{ode1} (Forward Euler).
  
\item[\textbf{Step Size}] For all benchmarks the step size in \simulink
  is fixed to $0.01$ milliseconds.  The same step size is also used in
  \ourTool, $\delta = 0.01$ milliseconds.
  
\item[\textbf{Time}] All benchmarks were simulated for $10$ seconds of
  simulation time.  Based on a step size of $0.01$ milliseconds this
  translates to $1$ million iterations.
  
\item[\textbf{Compiler}] All code was compiled using the \compiler
  compiler.  \ourTool code was compiled using both no optimisation
  (\texttt{O0}) and \texttt{O2} optimisation.  \simulink code was
  compiled using the automatically generated Makefile.
\end{description}

The experiments were evaluated using an Intel~i7-4790 processor with
8~GB RAM on Windows~7.


\subsection{Scalability}

\begin{figure}[htbp]
  \centering
  \input{./figures/scalabilityGraph}
  \caption{Scalability in  execution time of \simulink and \ourTool against 
  number of nodes in the \acf{NHN} model.}
  \label{fig:scalability}
\end{figure}

For the purposes of this experiment we aim to validate the scalability of
\ourTool through the running example of the \ac{NHN} whilst comparing it
to \simulink.  Code was generated for varying network sizes ($33$ nodes,
$66$ nodes, $99$ nodes, etc.) and the execution time recorded.  The
experimental set-up was the same as described in
Section~\ref{sec:experimentalSetUp}, ie. $1$ million iterations at a
$0.01$ millisecond step size.

The results are shown in Figure~\ref{fig:scalability}, with the most
obvious feature being that no data is recorded for \simulink for
complexities greater than $297$ nodes.  \simulink imposes an inbuilt
requirement that the generated code use less than $2$GB of memory. This
discontinuity represents the point after which the memory usage exceeds
this limit.\footnote{\simulink memory usage at a $297$ node network is
  $1.8$GB.}  \ourTool, on the other hand, is able to continue past this
point.

These results also illustrate that \ourTool has a smaller increase in
Execution Time as network size increases meaning that it is able to
maintain real-time with a model roughly $5$ times larger ($200$ nodes vs
$40$ nodes) than \simulink.  It is also of note that the change in
gradient of \ourTool around the $200$ node mark is due to the memory
usage exceeding the L$3$ cache size ($8$MB in our CPU).


\subsection{Diversity}
\label{sec:diversity}

\begin{figure}[htbp]
  \centering
  \subfigure[Normalised execution time relative to \simulink \label{fig:executionTime}]{
    \input{./figures/executionTimeGraph}
  }
  \subfigure[Executable size (KB) \label{fig:executableSize}]{
    \input{./figures/executableSizeGraph}
  }
  \caption{Comparison of the execution time (in ms) and executable size
    (in KB) between \simulink and \ourTool for the benchmarks in
    Table~\ref{tab:benchmarks}.}
  \label{fig:results}
\end{figure}

The purpose of the second experiment, we use the five benchmarks
presented in Table~\ref{tab:benchmarks}.  The table also presents the
number of locations (\#L) in each hybrid automata.  For example,
$(2^{50})$ denotes that the \acf{TSN} benchmark is described by $50$
instances of an \ac{HIOA} with two locations.

For all the benchmarks, the executable for the \simulink models are
generated using the in-built Real-time
Workshop\textsuperscript{\textregistered} C code generator.  Similarly,
for \ourTool, we generate equivalent C code, and compile it using a
standard C compiler.
The execution times and executable sizes of the generated programs are reported below and illustrated in Figure~\ref{fig:results}.\\

\textbf{Execution time:} Figure~\ref{fig:executionTime} shows that for
all benchmarks the execution time of \ourTool (both with no optimisation
and with optimisation level \texttt{O2}) is faster than that of
\simulink.  On average, we show that \ourTool is $9.8$ times faster than
\simulink.
For our most complicated example, the \ac{NHN}, we observe an improvement of 
$20.3$ times.\\

\textbf{Code size:} Figure~\ref{fig:executableSize} shows that the code
generated by \ourTool is also, generally, more compact than that
generated by \simulink.  On average, the optimised code of \ourTool is
$54\%$ smaller than \simulink when compiled.
For the \ac{NHN} example, the unoptimised code of \ourTool is comparable to 
that of \simulink while the optimised code sees improvements similar to that of 
the other benchmarks.\\

In summary, the code generated by \ourTool executes $9.8$ times faster
on average, with the executable size being $54\%$ smaller on average
when compared to \simulink.  \ \input{./content/05_tableBenchmarks}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../DATE2016_codegen"
%%% End:
