\section{Benchmarking}
\label{sec:benchmarking}


We present a set of experiments to evaluate the efficacy of the proposed modular code generation tool (\ourTool) with Simulink\textsuperscript{\textregistered}. 
In the first experiment, we evaluate the \emph{scalability} of \ourTool as the number of cells in the heart model increases. 
In the second experiment, we select benchmarks that span across different application domains such as medical, physics, and industrial automation, to illustrate the \emph{diversity} of the proposed approach.
We then compare each of these benchmarks against Simulink\textsuperscript{\textregistered} in terms of execution time and code size.


\subsection{Scalability}

\begin{figure}[htbp]
	\centering
	\input{./figures/scalabilityGraph}
	\caption{Scalability of \ourTool's execution time against number of cells}
	\label{fig:scalability}
\end{figure}

It scales!

\subsection{Diversity}

\begin{figure}[htbp]
	\centering
	\input{./figures/executionTimeGraph}
	\caption{Comparison of the execution time (in ms) between Simulink\textsuperscript{\textregistered} and \ourTool for the benchmarks in Table~\ref{tab:benchmarks}.}
	\label{fig:executionTime}
\end{figure}

For the purposes of this experiment, we use the five benchmarks presented in Table~\ref{tab:benchmarks}.
The table also presents the number of locations (\#L) in each hybrid automata.
For example, $(2, 2, 2)$ denotes that the \acf{TTS} benchmark is described using three \acp{HA} each with two locations.
More details about the benchmarks and their implementation in \ourTool and Simulink\textsuperscript{\textregistered} are available online~\cite{githubBenchmarks}.

\input{./content/05_tableBenchmarks}