\section{Benchmarking}
\label{sec:benchmarking}

We present a set of experiments to evaluate the efficacy of the proposed
modular code generation tool (\ourTool) against \simulink.  In the first 
experiment, we compare the \emph{scalability} of the two tools as the 
number of nodes in the \ac{NHN} model increases.  In the second experiment, we 
select benchmarks that span across different application domains 
%such as 
%medical, physics, and industrial automation 
to illustrate the \emph{diversity} 
of the proposed approach.  All benchmarks are 
available online~\cite{DATEBenchmarks}.


\subsection{Experimental set-up}
\label{sec:experimentalSetUp}
The following aspects were considered in order to achieve a fair
comparison between \ourTool and \simulink. 
(1) \textbf{Solver}: To reflect the synchronous execution model, we
  used a discrete numerical solver with a fixed step in \simulink,
  namely \texttt{ode1} (Forward Euler).
(2) 
\textbf{Step Size}: The fixed step size in \simulink and in \ourTool
  is  $0.01$~ms and $\delta = 0.01$~ms, respectively.
(3) 
\textbf{Time}: All benchmarks were simulated for $10$~seconds.
  With a step size of $0.01$~milliseconds this
  translates to $1$~million iterations in \ourTool.
  (4)
\textbf{Compiler}: All code was compiled using the \compiler
  compiler.  \ourTool code was compiled using both no optimisation
  (\texttt{O0}) and \texttt{O2} optimisation.  \simulink code was
  compiled using the automatically generated Makefile.
The experiments were evaluated using an Intel~i7-4790 processor with
$8$\,GB RAM on Windows~$7$.

\input{./content/05_tableBenchmarks}

\subsection{Scalability}
\vspace{-0.5cm}
\begin{figure}[htbp]
  \centering
  \input{./figures/scalabilityGraph}
  \setlength{\abovecaptionskip}{-10pt}
  \caption{Scalability in  execution time of \simulink and 
  \ourTool %against number of nodes in the \acf{NHN} model.
  }
  \label{fig:scalability}
  \vspace{-0.3cm}
\end{figure}

In this experiment, we  validate the scalability of
\ourTool through the running example of the \ac{NHN} whilst comparing it
to \simulink.  Code was generated for varying network sizes ($33$,
$66$, $99$~nodes, etc.) and the execution time recorded.  A correlation 
co-efficient of $0.9994$ was 
calculated between the output of \ourTool and \simulink, illustrating its 
correctness relative to \simulink.

The results are shown in Figure~\ref{fig:scalability}, 
%with the most
%obvious feature being that 
no data is recorded for \simulink for
complexities greater than $297$~nodes.  \simulink imposes an inbuilt
requirement that the generated code use less than $2$~GB of memory. This
discontinuity represents the point after which the memory usage exceeds
this limit ($1.8$\,GB).
%\footnote{\simulink memory usage at a $297$~node network is 
%}  
\ourTool, on the other hand, is able to continue past this
point.

These results also illustrate that \ourTool has a smaller increase in
Execution Time as network size increases. For the real time constraint of 
$10$~seconds for the benchmark, this means \ourTool is able to emulate a model 
roughly $5$~times larger ($200$~nodes vs $40$~nodes) than \simulink.  This 
can be seen through the line at $10$~seconds - the point at which the simulated 
time is equal to the execution time.  It is also of note that the change in 
gradient of \ourTool around the $200$~node mark is due to the memory usage 
exceeding the L$3$ cache size ($8$\,MB in our CPU).


\subsection{Diversity}
\label{sec:diversity}



For the second experiment, we use the  benchmarks
presented in Table~\ref{tab:benchmarks} where  
\#L represents the number of locations in each hybrid automata.  For example,
$(2^{50})$ denotes that the \acf{TSN} benchmark is described by $50$~instances 
of an \ac{HIOA} with two locations.

For all the benchmarks, the executable for the \simulink models are
generated using the in-built Real-time
Workshop\textsuperscript{\textregistered} `C' code generator.  Similarly,
for \ourTool, we generate equivalent `C' code.
The execution times and executable sizes of the generated programs are  
presented  in Figure~\ref{fig:results}.

\textbf{Execution time:} Figure~\ref{fig:executionTime} shows that for
all benchmarks the execution time of \ourTool (both without optimisation
and with optimisation level \texttt{O2}) is faster than that of
\simulink.  On average,  \ourTool is $9.8$~times faster than
\simulink.
For our most complicated example, the \ac{NHN}, we observe an improvement of 
$20.3$~times.

\textbf{Code size:} Figure~\ref{fig:executableSize} shows that the code
generated by \ourTool is also, generally, more compact than that
generated by \simulink.  On average, the optimised code of \ourTool is
$54\%$ smaller than \simulink when compiled.
For the \ac{NHN} example, the unoptimised code of \ourTool is comparable to 
that of \simulink while the optimised code sees improvements similar to that of 
the other benchmarks.

%In summary, the code generated by \ourTool executes $9.8$~times faster
%on average, with the executable size being $54\%$ smaller on average
%when compared to \simulink.

\begin{figure}[htbp]
	\centering
	\addtolength{\subfigcapskip}{-8pt}
	\subfigure[Normalised execution time relative to \simulink 
	\label{fig:executionTime}]{
		\input{./figures/executionTimeGraph}
	}
	\subfigure[Executable size (KB) \label{fig:executableSize}]{
		\input{./figures/executableSizeGraph}
	}
	\caption{Comparison of the execution time  and executable size
		%(in KB) between \simulink and \ourTool for the benchmarks in
		%Table~\ref{tab:benchmarks}.
		\label{fig:results}
	}
	\vspace{-0.4 cm}
\end{figure}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../DATE2016_codegen"
%%% End:
